{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "# import os\n",
        "\n",
        "if load_dotenv(find_dotenv()):\n",
        "    print(\"Environment variables loaded successfully!\") \n",
        "    # print(os.getenv('OPENAI_API_KEY'))\n",
        "else:\n",
        "    print(\"Could not load environment variables.\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings and vector databases are two important concepts in the field of natural language processing (NLP) and machine learning. They are closely related and play a crucial role in various applications such as information retrieval, recommendation systems, and semantic search. In this explanation, we will discuss the process of embeddings and vector databases and their relationship.\n",
            "\n",
            "1. Embeddings:\n",
            "Embeddings are vector representations of words, sentences, or documents in a high-dimensional space. The goal of embeddings is to capture the semantic and syntactic relationships between different entities in a way that is mathematically meaningful and computationally efficient. Embeddings are learned through a process called word embedding or sentence embedding.\n",
            "\n",
            "a. Word Embedding:\n",
            "Word embedding is the process of representing words as continuous vectors in a high-dimensional space. It is typically done using neural networks, specifically a technique called word2vec. Word2vec models learn word embeddings by training on large corpora of text data. The trained models capture the context and meaning of words by considering their surrounding words in the text. The resulting word embeddings can be used to measure semantic similarity, perform word analogy tasks, and support various NLP applications.\n",
            "\n",
            "b. Sentence Embedding:\n",
            "Sentence embedding is the process of representing sentences or documents as continuous vectors. Unlike word embeddings, sentence embeddings consider the entire context and meaning of a sentence rather than individual words. There are several techniques for sentence embedding, including averaging word embeddings, recurrent neural networks (RNNs), and transformers (such as BERT). These techniques capture the semantic relationships between sentences, enabling tasks like sentiment analysis, text classification, and information retrieval.\n",
            "\n",
            "2. Vector Databases:\n",
            "Vector databases, also known as similarity search databases, are specialized databases designed to efficiently store and query high-dimensional vectors. They are optimized for similarity search, which involves finding the most similar vectors to a given query vector. Vector databases use indexing structures and algorithms to organize and search the vectors effectively.\n",
            "\n",
            "a. Indexing Structures:\n",
            "Vector databases employ indexing structures such as k-d trees, ball trees, or random projection trees to partition the vector space. These structures divide the space into smaller regions, allowing for efficient search and retrieval of similar vectors. By organizing vectors in the index, vector databases can significantly speed up the similarity search process.\n",
            "\n",
            "b. Similarity Search Algorithms:\n",
            "Vector databases use similarity search algorithms to find the most similar vectors to a given query vector. These algorithms leverage the indexing structures to traverse the database efficiently and identify potential matches. Examples of similarity search algorithms include k-nearest neighbors (k-NN) search, approximate nearest neighbor (ANN) search, and locality-sensitive hashing (LSH). These algorithms trade off between accuracy and efficiency, providing different levels of approximation in the search process.\n",
            "\n",
            "3. Relationship between Embeddings and Vector Databases:\n",
            "Embeddings and vector databases are closely related in the sense that embeddings serve as the input data for vector databases. Embeddings capture the semantic relationships between words, sentences, or documents, while vector databases store and provide efficient search capabilities for these embeddings. The process of building a vector database typically involves the following steps:\n",
            "\n",
            "a. Embedding Generation:\n",
            "The first step is to generate embeddings for the entities of interest, such as words, sentences, or documents. This can be done using word embedding or sentence embedding techniques, as described earlier. The embeddings are then stored in a vector database.\n",
            "\n",
            "b. Indexing:\n",
            "The vector database organizes the embeddings using indexing structures to facilitate efficient search. The choice of indexing structure depends on the nature of the data and the desired trade-off between search time and memory usage.\n",
            "\n",
            "c. Querying:\n",
            "Once the vector database is built and indexed, it can be queried to find the most similar vectors to a given query vector. The query vector is typically an embedding of a word, sentence, or document that needs to be matched against the stored embeddings. The vector database employs similarity search algorithms to traverse the index and retrieve the most similar vectors.\n",
            "\n",
            "The relationship between embeddings and vector databases can be seen as a pipeline where embeddings are generated and then stored and queried using a vector database. This pipeline enables efficient retrieval of similar entities based on their semantic relationships, making it possible to build powerful applications such as semantic search engines, recommendation systems, and content-based filtering.\n",
            "\n",
            "In conclusion, embeddings and vector databases are essential components in the field of NLP and machine learning. Embeddings capture the semantic relationships between words, sentences, or documents, while vector databases provide efficient storage and retrieval mechanisms for these embeddings. Together, they enable advanced applications that require similarity search, content matching, and semantic analysis.\n"
          ]
        }
      ],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\") # can also pass through temperatue\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert computer scientist and developer.\"),\n",
        "    HumanMessage(content=\"Explain the process of embeddings and vector databases, and their relationship in at least 500 words.\")\n",
        "]\n",
        "response=chat(messages)\n",
        "# chat(messages)\n",
        "print(response.content,end='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Embeddings and vector databases are two important concepts in the field of natural language processing (NLP) and machine learning. They are closely related and play a crucial role in various applications such as information retrieval, recommendation systems, and semantic search. In this explanation, we will discuss the process of embeddings and vector databases and their relationship.\\n\\n1. Embeddings:\\nEmbeddings are vector representations of words, sentences, or documents in a high-dimensional space. The goal of embeddings is to capture the semantic and syntactic relationships between different entities in a way that is mathematically meaningful and computationally efficient. Embeddings are learned through a process called word embedding or sentence embedding.\\n\\na. Word Embedding:\\nWord embedding is the process of representing words as continuous vectors in a high-dimensional space. It is typically done using neural networks, specifically a technique called word2vec. Word2vec models learn word embeddings by training on large corpora of text data. The trained models capture the context and meaning of words by considering their surrounding words in the text. The resulting word embeddings can be used to measure semantic similarity, perform word analogy tasks, and support various NLP applications.\\n\\nb. Sentence Embedding:\\nSentence embedding is the process of representing sentences or documents as continuous vectors. Unlike word embeddings, sentence embeddings consider the entire context and meaning of a sentence rather than individual words. There are several techniques for sentence embedding, including averaging word embeddings, recurrent neural networks (RNNs), and transformers (such as BERT). These techniques capture the semantic relationships between sentences, enabling tasks like sentiment analysis, text classification, and information retrieval.\\n\\n2. Vector Databases:\\nVector databases, also known as similarity search databases, are specialized databases designed to efficiently store and query high-dimensional vectors. They are optimized for similarity search, which involves finding the most similar vectors to a given query vector. Vector databases use indexing structures and algorithms to organize and search the vectors effectively.\\n\\na. Indexing Structures:\\nVector databases employ indexing structures such as k-d trees, ball trees, or random projection trees to partition the vector space. These structures divide the space into smaller regions, allowing for efficient search and retrieval of similar vectors. By organizing vectors in the index, vector databases can significantly speed up the similarity search process.\\n\\nb. Similarity Search Algorithms:\\nVector databases use similarity search algorithms to find the most similar vectors to a given query vector. These algorithms leverage the indexing structures to traverse the database efficiently and identify potential matches. Examples of similarity search algorithms include k-nearest neighbors (k-NN) search, approximate nearest neighbor (ANN) search, and locality-sensitive hashing (LSH). These algorithms trade off between accuracy and efficiency, providing different levels of approximation in the search process.\\n\\n3. Relationship between Embeddings and Vector Databases:\\nEmbeddings and vector databases are closely related in the sense that embeddings serve as the input data for vector databases. Embeddings capture the semantic relationships between words, sentences, or documents, while vector databases store and provide efficient search capabilities for these embeddings. The process of building a vector database typically involves the following steps:\\n\\na. Embedding Generation:\\nThe first step is to generate embeddings for the entities of interest, such as words, sentences, or documents. This can be done using word embedding or sentence embedding techniques, as described earlier. The embeddings are then stored in a vector database.\\n\\nb. Indexing:\\nThe vector database organizes the embeddings using indexing structures to facilitate efficient search. The choice of indexing structure depends on the nature of the data and the desired trade-off between search time and memory usage.\\n\\nc. Querying:\\nOnce the vector database is built and indexed, it can be queried to find the most similar vectors to a given query vector. The query vector is typically an embedding of a word, sentence, or document that needs to be matched against the stored embeddings. The vector database employs similarity search algorithms to traverse the index and retrieve the most similar vectors.\\n\\nThe relationship between embeddings and vector databases can be seen as a pipeline where embeddings are generated and then stored and queried using a vector database. This pipeline enables efficient retrieval of similar entities based on their semantic relationships, making it possible to build powerful applications such as semantic search engines, recommendation systems, and content-based filtering.\\n\\nIn conclusion, embeddings and vector databases are essential components in the field of NLP and machine learning. Embeddings capture the semantic relationships between words, sentences, or documents, while vector databases provide efficient storage and retrieval mechanisms for these embeddings. Together, they enable advanced applications that require similarity search, content matching, and semantic analysis.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_str=response.content\n",
        "response_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Embeddings and vector databases are two important concepts in the field of natural language', metadata={}),\n",
              " Document(page_content='processing (NLP) and machine learning. They are closely related and play a crucial role in various', metadata={}),\n",
              " Document(page_content='applications such as information retrieval, recommendation systems, and semantic search. In this', metadata={}),\n",
              " Document(page_content='explanation, we will discuss the process of embeddings and vector databases and their relationship.', metadata={}),\n",
              " Document(page_content='1. Embeddings:', metadata={}),\n",
              " Document(page_content='Embeddings are vector representations of words, sentences, or documents in a high-dimensional', metadata={}),\n",
              " Document(page_content='space. The goal of embeddings is to capture the semantic and syntactic relationships between', metadata={}),\n",
              " Document(page_content='different entities in a way that is mathematically meaningful and computationally efficient.', metadata={}),\n",
              " Document(page_content='Embeddings are learned through a process called word embedding or sentence embedding.', metadata={}),\n",
              " Document(page_content='a. Word Embedding:', metadata={}),\n",
              " Document(page_content='Word embedding is the process of representing words as continuous vectors in a high-dimensional', metadata={}),\n",
              " Document(page_content='space. It is typically done using neural networks, specifically a technique called word2vec.', metadata={}),\n",
              " Document(page_content='Word2vec models learn word embeddings by training on large corpora of text data. The trained models', metadata={}),\n",
              " Document(page_content='capture the context and meaning of words by considering their surrounding words in the text. The', metadata={}),\n",
              " Document(page_content='resulting word embeddings can be used to measure semantic similarity, perform word analogy tasks,', metadata={}),\n",
              " Document(page_content='and support various NLP applications.', metadata={}),\n",
              " Document(page_content='b. Sentence Embedding:', metadata={}),\n",
              " Document(page_content='Sentence embedding is the process of representing sentences or documents as continuous vectors.', metadata={}),\n",
              " Document(page_content='Unlike word embeddings, sentence embeddings consider the entire context and meaning of a sentence', metadata={}),\n",
              " Document(page_content='rather than individual words. There are several techniques for sentence embedding, including', metadata={}),\n",
              " Document(page_content='averaging word embeddings, recurrent neural networks (RNNs), and transformers (such as BERT). These', metadata={}),\n",
              " Document(page_content='techniques capture the semantic relationships between sentences, enabling tasks like sentiment', metadata={}),\n",
              " Document(page_content='analysis, text classification, and information retrieval.', metadata={}),\n",
              " Document(page_content='2. Vector Databases:', metadata={}),\n",
              " Document(page_content='Vector databases, also known as similarity search databases, are specialized databases designed to', metadata={}),\n",
              " Document(page_content='efficiently store and query high-dimensional vectors. They are optimized for similarity search,', metadata={}),\n",
              " Document(page_content='which involves finding the most similar vectors to a given query vector. Vector databases use', metadata={}),\n",
              " Document(page_content='indexing structures and algorithms to organize and search the vectors effectively.', metadata={}),\n",
              " Document(page_content='a. Indexing Structures:', metadata={}),\n",
              " Document(page_content='Vector databases employ indexing structures such as k-d trees, ball trees, or random projection', metadata={}),\n",
              " Document(page_content='trees to partition the vector space. These structures divide the space into smaller regions,', metadata={}),\n",
              " Document(page_content='allowing for efficient search and retrieval of similar vectors. By organizing vectors in the index,', metadata={}),\n",
              " Document(page_content='vector databases can significantly speed up the similarity search process.', metadata={}),\n",
              " Document(page_content='b. Similarity Search Algorithms:', metadata={}),\n",
              " Document(page_content='Vector databases use similarity search algorithms to find the most similar vectors to a given query', metadata={}),\n",
              " Document(page_content='vector. These algorithms leverage the indexing structures to traverse the database efficiently and', metadata={}),\n",
              " Document(page_content='identify potential matches. Examples of similarity search algorithms include k-nearest neighbors', metadata={}),\n",
              " Document(page_content='(k-NN) search, approximate nearest neighbor (ANN) search, and locality-sensitive hashing (LSH).', metadata={}),\n",
              " Document(page_content='These algorithms trade off between accuracy and efficiency, providing different levels of', metadata={}),\n",
              " Document(page_content='approximation in the search process.', metadata={}),\n",
              " Document(page_content='3. Relationship between Embeddings and Vector Databases:', metadata={}),\n",
              " Document(page_content='Embeddings and vector databases are closely related in the sense that embeddings serve as the input', metadata={}),\n",
              " Document(page_content='data for vector databases. Embeddings capture the semantic relationships between words, sentences,', metadata={}),\n",
              " Document(page_content='or documents, while vector databases store and provide efficient search capabilities for these', metadata={}),\n",
              " Document(page_content='embeddings. The process of building a vector database typically involves the following steps:', metadata={}),\n",
              " Document(page_content='a. Embedding Generation:', metadata={}),\n",
              " Document(page_content='The first step is to generate embeddings for the entities of interest, such as words, sentences, or', metadata={}),\n",
              " Document(page_content='documents. This can be done using word embedding or sentence embedding techniques, as described', metadata={}),\n",
              " Document(page_content='earlier. The embeddings are then stored in a vector database.', metadata={}),\n",
              " Document(page_content='b. Indexing:', metadata={}),\n",
              " Document(page_content='The vector database organizes the embeddings using indexing structures to facilitate efficient', metadata={}),\n",
              " Document(page_content='search. The choice of indexing structure depends on the nature of the data and the desired', metadata={}),\n",
              " Document(page_content='trade-off between search time and memory usage.', metadata={}),\n",
              " Document(page_content='c. Querying:', metadata={}),\n",
              " Document(page_content='Once the vector database is built and indexed, it can be queried to find the most similar vectors', metadata={}),\n",
              " Document(page_content='to a given query vector. The query vector is typically an embedding of a word, sentence, or', metadata={}),\n",
              " Document(page_content='document that needs to be matched against the stored embeddings. The vector database employs', metadata={}),\n",
              " Document(page_content='similarity search algorithms to traverse the index and retrieve the most similar vectors.', metadata={}),\n",
              " Document(page_content='The relationship between embeddings and vector databases can be seen as a pipeline where embeddings', metadata={}),\n",
              " Document(page_content='are generated and then stored and queried using a vector database. This pipeline enables efficient', metadata={}),\n",
              " Document(page_content='retrieval of similar entities based on their semantic relationships, making it possible to build', metadata={}),\n",
              " Document(page_content='powerful applications such as semantic search engines, recommendation systems, and content-based', metadata={}),\n",
              " Document(page_content='filtering.', metadata={}),\n",
              " Document(page_content='In conclusion, embeddings and vector databases are essential components in the field of NLP and', metadata={}),\n",
              " Document(page_content='machine learning. Embeddings capture the semantic relationships between words, sentences, or', metadata={}),\n",
              " Document(page_content='documents, while vector databases provide efficient storage and retrieval mechanisms for these', metadata={}),\n",
              " Document(page_content='embeddings. Together, they enable advanced applications that require similarity search, content', metadata={}),\n",
              " Document(page_content='matching, and semantic analysis.', metadata={})]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([response_str])\n",
        "texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings and vector databases are two important concepts in the field of natural language\n",
            "processing (NLP) and machine learning. They are closely related and play a crucial role in various\n",
            "applications such as information retrieval, recommendation systems, and semantic search. In this\n",
            "explanation, we will discuss the process of embeddings and vector databases and their relationship.\n",
            "1. Embeddings:\n",
            "Embeddings are vector representations of words, sentences, or documents in a high-dimensional\n",
            "space. The goal of embeddings is to capture the semantic and syntactic relationships between\n",
            "different entities in a way that is mathematically meaningful and computationally efficient.\n",
            "Embeddings are learned through a process called word embedding or sentence embedding.\n",
            "a. Word Embedding:\n",
            "Word embedding is the process of representing words as continuous vectors in a high-dimensional\n",
            "space. It is typically done using neural networks, specifically a technique called word2vec.\n",
            "Word2vec models learn word embeddings by training on large corpora of text data. The trained models\n",
            "capture the context and meaning of words by considering their surrounding words in the text. The\n",
            "resulting word embeddings can be used to measure semantic similarity, perform word analogy tasks,\n",
            "and support various NLP applications.\n",
            "b. Sentence Embedding:\n",
            "Sentence embedding is the process of representing sentences or documents as continuous vectors.\n",
            "Unlike word embeddings, sentence embeddings consider the entire context and meaning of a sentence\n",
            "rather than individual words. There are several techniques for sentence embedding, including\n",
            "averaging word embeddings, recurrent neural networks (RNNs), and transformers (such as BERT). These\n",
            "techniques capture the semantic relationships between sentences, enabling tasks like sentiment\n",
            "analysis, text classification, and information retrieval.\n",
            "2. Vector Databases:\n",
            "Vector databases, also known as similarity search databases, are specialized databases designed to\n",
            "efficiently store and query high-dimensional vectors. They are optimized for similarity search,\n",
            "which involves finding the most similar vectors to a given query vector. Vector databases use\n",
            "indexing structures and algorithms to organize and search the vectors effectively.\n",
            "a. Indexing Structures:\n",
            "Vector databases employ indexing structures such as k-d trees, ball trees, or random projection\n",
            "trees to partition the vector space. These structures divide the space into smaller regions,\n",
            "allowing for efficient search and retrieval of similar vectors. By organizing vectors in the index,\n",
            "vector databases can significantly speed up the similarity search process.\n",
            "b. Similarity Search Algorithms:\n",
            "Vector databases use similarity search algorithms to find the most similar vectors to a given query\n",
            "vector. These algorithms leverage the indexing structures to traverse the database efficiently and\n",
            "identify potential matches. Examples of similarity search algorithms include k-nearest neighbors\n",
            "(k-NN) search, approximate nearest neighbor (ANN) search, and locality-sensitive hashing (LSH).\n",
            "These algorithms trade off between accuracy and efficiency, providing different levels of\n",
            "approximation in the search process.\n",
            "3. Relationship between Embeddings and Vector Databases:\n",
            "Embeddings and vector databases are closely related in the sense that embeddings serve as the input\n",
            "data for vector databases. Embeddings capture the semantic relationships between words, sentences,\n",
            "or documents, while vector databases store and provide efficient search capabilities for these\n",
            "embeddings. The process of building a vector database typically involves the following steps:\n",
            "a. Embedding Generation:\n",
            "The first step is to generate embeddings for the entities of interest, such as words, sentences, or\n",
            "documents. This can be done using word embedding or sentence embedding techniques, as described\n",
            "earlier. The embeddings are then stored in a vector database.\n",
            "b. Indexing:\n",
            "The vector database organizes the embeddings using indexing structures to facilitate efficient\n",
            "search. The choice of indexing structure depends on the nature of the data and the desired\n",
            "trade-off between search time and memory usage.\n",
            "c. Querying:\n",
            "Once the vector database is built and indexed, it can be queried to find the most similar vectors\n",
            "to a given query vector. The query vector is typically an embedding of a word, sentence, or\n",
            "document that needs to be matched against the stored embeddings. The vector database employs\n",
            "similarity search algorithms to traverse the index and retrieve the most similar vectors.\n",
            "The relationship between embeddings and vector databases can be seen as a pipeline where embeddings\n",
            "are generated and then stored and queried using a vector database. This pipeline enables efficient\n",
            "retrieval of similar entities based on their semantic relationships, making it possible to build\n",
            "powerful applications such as semantic search engines, recommendation systems, and content-based\n",
            "filtering.\n",
            "In conclusion, embeddings and vector databases are essential components in the field of NLP and\n",
            "machine learning. Embeddings capture the semantic relationships between words, sentences, or\n",
            "documents, while vector databases provide efficient storage and retrieval mechanisms for these\n",
            "embeddings. Together, they enable advanced applications that require similarity search, content\n",
            "matching, and semantic analysis.\n"
          ]
        }
      ],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "# texts[0].page_content\n",
        "for i in range(0, len(texts)):\n",
        "    print(texts[i].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for OpenAIEmbeddings\nmodel_name\n  extra fields not permitted (type=value_error.extra)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Import and instantiate OpenAI embeddings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m----> 3\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mada\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n\u001b[1;32m      4\u001b[0m embeddings\n",
            "File \u001b[0;32m~/cs-projects/school/cs467-capstone/myenv/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\nmodel_name\n  extra fields not permitted (type=value_error.extra)"
          ]
        }
      ],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model_name=\"text-embedding-ada-002\") \n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "# print(query_result)\n",
        "\n",
        "# todo: avail functions for OpenAIEmbeddings? should this be embed_query, or document/search, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
        "    environment=os.getenv('PINECONE_ENV')  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "index_name = \"learn-langchain\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='context of a word and its actual context.', metadata={}),\n",
              " Document(page_content='detection, and search engines.', metadata={}),\n",
              " Document(page_content='approximate similarity search.', metadata={}),\n",
              " Document(page_content='embedding space.', metadata={})]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do a simple vector similarity search\n",
        "query = \"What is the purpose of a vector?\"\n",
        "result = search.similarity_search(query)\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
